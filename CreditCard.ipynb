{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpN58j8zkyVmKWSmGavFP5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khushi-dahiya08/CreditCard/blob/main/CreditCard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTEENN\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"Creditcard_data.csv\")\n",
        "\n",
        "# Balancing the dataset using undersampling\n",
        "def balance_dataset(data):\n",
        "    X = data.drop(columns=['Class'])\n",
        "    y = data['Class']\n",
        "    rus = RandomUnderSampler(random_state=42)\n",
        "    X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "    return pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Class'])], axis=1)\n",
        "\n",
        "balanced_data = balance_dataset(data)\n",
        "\n",
        "# Dynamic sample size\n",
        "available_data_size = len(balanced_data)\n",
        "sample_size = min(100, available_data_size)\n",
        "\n",
        "# sampling methods\n",
        "def random_sampling(data, size):\n",
        "    return data.sample(n=size, random_state=42) if size <= len(data) else data\n",
        "\n",
        "def systematic_sampling(data, size):\n",
        "    step = len(data) // size if size <= len(data) else 1\n",
        "    return data.iloc[::step][:size]\n",
        "\n",
        "def stratified_sampling(data, size):\n",
        "    groups = data.groupby('Class')\n",
        "    return groups.apply(lambda x: x.sample(int(size / len(groups)), random_state=42))[:size]\n",
        "\n",
        "def smote_sampling(data, size):\n",
        "    X = data.drop(columns=['Class'])\n",
        "    y = data['Class']\n",
        "    smote = SMOTE(random_state=42, sampling_strategy=min(size / len(data), 1.0))\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    return pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Class'])], axis=1)\n",
        "\n",
        "def smote_enn_sampling(data, size):\n",
        "    X = data.drop(columns=['Class'])\n",
        "    y = data['Class']\n",
        "    smote_enn = SMOTEENN(random_state=42)\n",
        "    X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
        "    if len(X_resampled) < size:\n",
        "        size = len(X_resampled)\n",
        "    return pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Class'])], axis=1).sample(n=size, random_state=42)\n",
        "\n",
        "# Sampling methods\n",
        "sampling_methods = [random_sampling, systematic_sampling, stratified_sampling, smote_sampling, smote_enn_sampling]\n",
        "sampling_names = ['Random Sampling', 'Systematic Sampling', 'Stratified Sampling', 'SMOTE Sampling', 'SMOTE-ENN Sampling']\n",
        "\n",
        "#  machine learning models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"SVM\": SVC(random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Running sampling methods on models\n",
        "results = []\n",
        "\n",
        "for i, sampling_method in enumerate(sampling_methods):\n",
        "    try:\n",
        "        print(f\"Applying {sampling_names[i]}...\")\n",
        "        sampled_data = sampling_method(balanced_data, size=sample_size)\n",
        "        X_sample = sampled_data.drop(columns=['Class'])\n",
        "        y_sample = sampled_data['Class']\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n",
        "\n",
        "        for model_name, model in models.items():\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            results.append({\n",
        "                \"Sampling Method\": sampling_names[i],\n",
        "                \"Model\": model_name,\n",
        "                \"Accuracy\": accuracy\n",
        "            })\n",
        "            print(f\"{sampling_names[i]} + {model_name}: Accuracy = {accuracy:.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in {sampling_names[i]}: {e}\")\n",
        "\n",
        "# Converting results to DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nSummary of Results:\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VadJW5tcZYsU",
        "outputId": "14d7d533-1630-41ac-a6cb-987869f7897d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying Random Sampling...\n",
            "Random Sampling + Logistic Regression: Accuracy = 0.50\n",
            "Random Sampling + Random Forest: Accuracy = 0.50\n",
            "Random Sampling + SVM: Accuracy = 0.75\n",
            "Random Sampling + KNN: Accuracy = 0.50\n",
            "Random Sampling + Decision Tree: Accuracy = 0.50\n",
            "Applying Systematic Sampling...\n",
            "Systematic Sampling + Logistic Regression: Accuracy = 0.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Systematic Sampling + Random Forest: Accuracy = 0.25\n",
            "Systematic Sampling + SVM: Accuracy = 0.00\n",
            "Systematic Sampling + KNN: Accuracy = 0.25\n",
            "Systematic Sampling + Decision Tree: Accuracy = 0.75\n",
            "Applying Stratified Sampling...\n",
            "Stratified Sampling + Logistic Regression: Accuracy = 0.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-7de329405087>:41: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  return groups.apply(lambda x: x.sample(int(size / len(groups)), random_state=42))[:size]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stratified Sampling + Random Forest: Accuracy = 0.00\n",
            "Stratified Sampling + SVM: Accuracy = 0.00\n",
            "Stratified Sampling + KNN: Accuracy = 0.25\n",
            "Stratified Sampling + Decision Tree: Accuracy = 0.75\n",
            "Applying SMOTE Sampling...\n",
            "Error in SMOTE Sampling: The specified ratio required to remove samples from the minority class while trying to generate new samples. Please increase the ratio.\n",
            "Applying SMOTE-ENN Sampling...\n",
            "Error in SMOTE-ENN Sampling: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.\n",
            "\n",
            "Summary of Results:\n",
            "        Sampling Method                Model  Accuracy\n",
            "0       Random Sampling  Logistic Regression      0.50\n",
            "1       Random Sampling        Random Forest      0.50\n",
            "2       Random Sampling                  SVM      0.75\n",
            "3       Random Sampling                  KNN      0.50\n",
            "4       Random Sampling        Decision Tree      0.50\n",
            "5   Systematic Sampling  Logistic Regression      0.25\n",
            "6   Systematic Sampling        Random Forest      0.25\n",
            "7   Systematic Sampling                  SVM      0.00\n",
            "8   Systematic Sampling                  KNN      0.25\n",
            "9   Systematic Sampling        Decision Tree      0.75\n",
            "10  Stratified Sampling  Logistic Regression      0.25\n",
            "11  Stratified Sampling        Random Forest      0.00\n",
            "12  Stratified Sampling                  SVM      0.00\n",
            "13  Stratified Sampling                  KNN      0.25\n",
            "14  Stratified Sampling        Decision Tree      0.75\n"
          ]
        }
      ]
    }
  ]
}